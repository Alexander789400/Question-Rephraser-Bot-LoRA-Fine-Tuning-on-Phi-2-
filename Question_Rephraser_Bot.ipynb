{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "834816ac",
   "metadata": {},
   "source": [
    "# Question Rephraser Bot\n",
    "\n",
    "Problem Statement :\n",
    "\n",
    "The goal of this project is to build a Question Rephraser Bot that can rewrite user questions into a clearer, more formal, and grammatically correct form.\n",
    "\n",
    "Many user queries on the internet are often written in an informal or unclear way, such as:\n",
    "\n",
    "“wat is best way to learn ai fast?”\n",
    "\n",
    "The model should generate a refined version like:\n",
    "\n",
    "“What is the most effective way to learn AI quickly?”\n",
    "\n",
    "To achieve this, we fine-tune the Phi-2 language model using the Quora Question Pairs dataset, where duplicate question pairs act as paraphrase examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07d970fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -q transformers datasets peft trl accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c1c78",
   "metadata": {},
   "source": [
    "### Load Quora Question Pairs Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acf09ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"AlekseyKorshuk/quora-question-pairs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "178c3a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate'],\n",
      "        num_rows: 404290\n",
      "    })\n",
      "})\n",
      "{'id': 0, 'qid1': 1, 'qid2': 2, 'question1': 'What is the step by step guide to invest in share market in india?', 'question2': 'What is the step by step guide to invest in share market?', 'is_duplicate': 0}\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "print(ds[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19630ecd",
   "metadata": {},
   "source": [
    "### Instruction-Response Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59589a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_instruction(example):\n",
    "    q1 = example[\"question1\"]\n",
    "    q2 = example[\"question2\"]\n",
    "\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Rewrite the following question in a clearer and more formal way.\n",
    "\n",
    "### Input:\n",
    "{q1}\n",
    "\n",
    "### Response:\n",
    "{q2}\"\"\"\n",
    "\n",
    "    return prompt   # ✅ Return STRING only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858d0d86",
   "metadata": {},
   "source": [
    "### Filter Only Duplicate Question Pairs\n",
    "\n",
    "Meaning of is_duplicate\n",
    "\n",
    "1 → Both questions mean the same thing (We only want real paraphrases)\n",
    "\n",
    "0 → Both questions are completely different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14503dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate pairs: 149263\n"
     ]
    }
   ],
   "source": [
    "filtered_ds = ds[\"train\"].filter(lambda x: x[\"is_duplicate\"] == 1)\n",
    "\n",
    "print(\"Total duplicate pairs:\", len(filtered_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa5b8494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149263, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed392ee1",
   "metadata": {},
   "source": [
    "### Reduce Dataset for Quick Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32b63081",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_ds = filtered_ds.select(range(200))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a61629",
   "metadata": {},
   "source": [
    "### Load Phi-2 Model + Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09d0f3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The 8-bit optimizer is not available on your device, only available on CUDA for now.\n",
      "The 8-bit optimizer is not available on your device, only available on CUDA for now.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:04<00:00, 62.35s/it] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model in 4-bit\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31aefbb",
   "metadata": {},
   "source": [
    "### Apply LoRA Fine-Tuning Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dccf0f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,621,440 || all params: 2,782,305,280 || trainable%: 0.0942\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a9c78f",
   "metadata": {},
   "source": [
    "### Train with SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b242a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./phi2-question-rephraser\",\n",
    "\n",
    "    per_device_train_batch_size=1,  \n",
    "    gradient_accumulation_steps=1,   # no accumulation = quickest\n",
    "\n",
    "    num_train_epochs=1,     # keep only 1 epoch\n",
    "    max_steps=20,\n",
    "\n",
    "    learning_rate=2e-4,\n",
    "\n",
    "    logging_steps=10,\n",
    "    save_steps=500,                 # save less often = faster\n",
    "\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76477d4a",
   "metadata": {},
   "source": [
    "#### Supervised Fine-Tuning Trainer  (SFTTrainer)\n",
    "\n",
    "It is a special training class from the TRL library (trl) that makes it very easy to fine-tune large language models (LLMs) on instruction datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05b36911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer Setup\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=filtered_ds,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    "    formatting_func= convert_to_instruction\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eadc0103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n",
      "c:\\Users\\Pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 32:11, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.636400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.218700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=2.427521514892578, metrics={'train_runtime': 2030.4327, 'train_samples_per_second': 0.01, 'train_steps_per_second': 0.01, 'total_flos': 17370880450560.0, 'train_loss': 2.427521514892578})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e3fa0f",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54ec991d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=60\n",
    ")\n",
    "\n",
    "def rephrase(question):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "Rewrite the following question in a clearer and more formal way.\n",
    "\n",
    "### Input:\n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "    output = pipe(prompt)[0][\"generated_text\"]\n",
    "    return output.split(\"### Response:\")[-1].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4803361",
   "metadata": {},
   "source": [
    "### Custom Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df0b2772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which laptop would you recommend for a computer science student?\n",
      "What are the best ways to apply for a job at Google?\n",
      "AI and ML are different in terms of the type of tasks they can perform. AI can be either narrow or general, depending on the domain of the problem. ML can be either supervised or unsupervised, depending on the availability of labeled data.\n",
      "How to prepare for Google Interview?\n",
      "How to write a resume for freshers?\n",
      "What is the most efficient way to learn machine learning quickly?\n",
      "Which programming language is most suitable for beginners?\n",
      "How can I improve my CGPA in engineering?\n",
      "Can you explain neural networks in simple words?\n",
      "- How can one maintain a positive mindset and drive while acquiring knowledge and skills in the field of coding?\n",
      "What is the reason for the Python code not to run?\n"
     ]
    }
   ],
   "source": [
    "print(rephrase(\"wat is best laptop for ai student\"))\n",
    "print(rephrase(\"how to get job in google fast\"))\n",
    "print(rephrase(\"what is difference between ai and ml??\"))\n",
    "print(rephrase(\"how to crack google interview fast??\"))\n",
    "print(rephrase(\"how to build resume for freshers\"))\n",
    "print(rephrase(\"tell me best way to learn machine learning quickly\"))\n",
    "print(rephrase(\"which programming language is best for beginners?\"))\n",
    "print(rephrase(\"how to increase cgpa in engineering\"))\n",
    "print(rephrase(\"can u explain neural networks in simple words\"))\n",
    "print(rephrase(\"how to stay motivated while learning coding\"))\n",
    "print(rephrase(\"why my python code not running\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8876e207",
   "metadata": {},
   "source": [
    "#### What is LoRA ?\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique used to adapt large language models without updating all model weights.\n",
    "\n",
    "Instead of modifying the full model, LoRA introduces small trainable matrices into specific layers (such as attention layers).\n",
    "\n",
    "Key idea:\n",
    "\n",
    "The original model remains frozen\n",
    "\n",
    "Only a small number of additional parameters are trained\n",
    "\n",
    "This makes training faster and lighter\n",
    "\n",
    "LoRA allows large models to learn new tasks efficiently with minimal compute requirements.\n",
    "\n",
    "#### Why LoRA Instead of Full Fine-Tuning ?\n",
    "\n",
    "Full fine-tuning updates all parameters of a large model, which is expensive and requires high GPU memory.\n",
    "\n",
    "LoRA is preferred because:\n",
    "\n",
    "* Lower memory usage (only small adapters are trained)\n",
    "\n",
    "* Faster training compared to full fine-tuning\n",
    "\n",
    "* Cheaper compute cost\n",
    "\n",
    "* Prevents overfitting when dataset size is limited\n",
    "\n",
    "* Makes deployment easier since LoRA adapters are lightweight\n",
    "\n",
    "Thus, LoRA is an efficient approach for fine-tuning models like Phi-2 on tasks such as question rewriting.\n",
    "\n",
    "### Observations from the Results :\n",
    "\n",
    "After training Phi-2 with LoRA on duplicate question pairs:\n",
    "\n",
    "The model successfully learned to rewrite informal questions into more structured and formal language.\n",
    "\n",
    "Output questions became clearer, grammatically correct, and closer to professional phrasing.\n",
    "\n",
    "The model performed well on common user queries such as job, education, and programming questions.\n",
    "\n",
    "Example:\n",
    "\n",
    "Input:\n",
    "\n",
    "“why my python code not running??”\n",
    "\n",
    "Output:\n",
    "\n",
    "“Why is my Python code not executing properly?”\n",
    "\n",
    "Limitations Observed:\n",
    "\n",
    "Occasionally, the model produces outputs too similar to the input if the question is already formal.\n",
    "\n",
    "More training epochs and a larger dataset can further improve paraphrasing quality.\n",
    "\n",
    "Overall, LoRA fine-tuning proved effective for building a lightweight question rephrasing assistant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
